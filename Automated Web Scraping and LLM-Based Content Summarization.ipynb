{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33352cf0",
   "metadata": {},
   "source": [
    "# Scrap the html content from website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf79d6",
   "metadata": {},
   "source": [
    "## Step_1 : extract the content using requests module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63caa09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(url=\"https://blog.londonappbrewery.com/how-a-doctor-ran-a-tech-startup-while-working-80-hour-weeks-d47e7b4988cb\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ec935a",
   "metadata": {},
   "source": [
    "## Step_2 : Using Bs4 the html content is formated properly with help of prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1acdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "print(soup.prettify())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01313f81",
   "metadata": {},
   "source": [
    "### a. using find_all method extract only necessary content and stored in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = [\"h1\",\"h2\",\"h3\",\"p\"]\n",
    "li = soup.find_all(tags_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c092996",
   "metadata": {},
   "source": [
    "### b. using that \"li\" list extract each individuial line text and store in list \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5c2ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "website = []\n",
    "for i in li:\n",
    "    website.append(i.get_text())\n",
    "website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a76ae8",
   "metadata": {},
   "source": [
    "# Train the llm model using the scraped data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab0d78",
   "metadata": {},
   "source": [
    "## Setup llama and import model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b1cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get(\"http://localhost:11434\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81690d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84583d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a professional content summarizer. Your task is to read the text provided by the user and generate a concise, clear, and accurate summary. Follow these rules:\n",
    "\n",
    "1. Identify and include only the main ideas and key points.\n",
    "2. Remove any redundant, trivial, or irrelevant information.\n",
    "3. Preserve the original meaning and tone of the text.\n",
    "4. Make the summary easy to read and understand.\n",
    "5. Provide the summary in [choose format: paragraph / bullet points / numbered list] as requested by the user.\n",
    "6. Keep the summary concise, ideally under [user-specified length, e.g., 150 words].\n",
    "\n",
    "Wait for the user to provide the text to summarize.\n",
    "\n",
    "Respond in markdown. Do not wrap the markdown in a code block - respond just with the markdown.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2853b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our user prompt\n",
    "\n",
    "user_prompt_prefix = \"\"\"\n",
    "Here are the contents of a website.\n",
    "Provide a short summary of this website.\n",
    "If it includes news or announcements, then summarize these too.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a065cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_prefix + website}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5770bed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "website_content = ','.join(website)\n",
    "website_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_for(website=website_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d5bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "# def summarize(url):\n",
    "#     response = ollama.chat.completions.create(\n",
    "#         model = \"gpt-4.1-mini\",\n",
    "#         messages = messages_for(content)\n",
    "#     )\n",
    "#     return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"mistral:latest\", messages=messages_for(website=website_content))\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat.completions.create(model=\"mistral:latest\", messages=[{\"role\": \"user\", \"content\": \"Tell me a fun fact\"}])\n",
    "\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c963f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.12.12 (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
